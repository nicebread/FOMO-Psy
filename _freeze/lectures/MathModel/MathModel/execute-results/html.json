{
  "hash": "9491cf6f63674239fa67d3b27907abcb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mathematical Modeling of Psychological Theories\"\nsubtitle: \"Introduction\"\nauthor:\n  - name: Felix Schönbrodt\n    orcid: 0000-0002-8282-3910\n    email: felix.schoenbrodt@psy.lmu.de\n    affiliations: Ludwig-Maximilians-Universität München\ndate: 2024-11-08\nfooter: \"Formal modeling in psychology - Empirisches Praktikum, Ludwig-Maximilians-Universität München\"\nformat: \n  nicetheme-revealjs: \n    output-ext: slide.html\n  html: default\nrevealjs-plugins:\n  - attribution \n---\n\n\n\n\n\n\n\n\n\n\n##\n![](img/How_deep_is_your_love.png){height='450' style='filter: drop-shadow(6px 6px 8px #555555);'}\n\n::: {.fragment}\n::: {.callout-warning title=\"**Answer: -136.7**\"}\n:::\n:::\n\n\n# Step 1: Define variables\n\n## Define variables\n### How deep is your love?\nDefine a variable for each construct of the VAST display. These can be *measured* variables or *unmeasured* (mediating) variables. \n\nYou can either refer to an actual measurement procedure or simply define a variable. In both cases you should explicitly define the following properties:\n\n- *variable name* (i.e., to which concept in your VAST display does it refer?),\n- *scale level* (categorical, ordinal, interval scale), \n- *range* of possible values (e.g., 0 ... 1), \n- *semantic anchors* (e.g., 0 = \"complete absence\", 1 = \"maximal possible value\")\n\n## Define variables\n### Some guiding questions and heuristics\n\n- Type of variable: continuous, dichotomous, ordinal, categorical?\n- Scale level of measurement ([Stevens's typology](https://en.wikipedia.org/wiki/Level_of_measurement)): <br>Nominal &rarr; Ordinal &rarr; Interval &rarr; Ratio\n- Is the variable naturally bounded? On both sides or only one?\n- How can the numbers be interpreted?\n  - Natural/objective scale (e.g. physical distance)\n  - As standardized *z*-scores?\n  - Normalized to a value between 0 and 1? Or rather -1 to 1?\n  - Can we find an empirical or semantic calibration?\n    - Just noticable difference\n    - 100 = \"largest realistically imaginable quantity of that variable\"\n\n\n\n## {{< fa people-group size=2x >}} Group work (15 min.): Specify variables\n\n::: {.smaller}\nIn the Google doc, below your *Construct Source Table*, create a new *Variables Table* with the following columns:\n\n*Example:*\n\n<div class=\"table-grid smaller\">\n| Construct in VAST display              | Shortname   | Type | Range/values | Anchors                                                           |\n| -------------------------------------- | ----------- | ----------- | ------------ | ----------------------------------------------------------------- |\n| Affective tone of instruction          | aff_tone    | Continuous  | [-1; 1]      | -1 = maximally negative<br>0 = neutral<br>+1 = maximally positive |\n| Anxiety                                | anxiety     | Continuous  | [0; 1]       | 0 = no anxiety<br>1 = maximal anxiety                             |\n| Kohlberg's Stages of Moral Development | moral_stage | Ordinal     | {1; 2; 3}    | 1=Pre-conventional<br>2=Conventional<br>3=Post-Conventional       |\n| ...                                    |             | ...         | ...          | ...                                                               |\n</div>\n\nNote: This resembles a **codebook**; but for theoretical variables, not only for measured variables.\nAs a heuristic, list **all concepts that are not higher-order concepts** (because these usually have no single numerical representation).\n:::\n\n# Step 2: Define functional relationships between variables\n\n## {{< fa people-group size=2x >}} Group work (10 min.)\n### Sketch a first functional relationship on paper\n\nWe want to model the following **phenomenon** (a specific version of the bystander effect):\n\n1. Without other people present, the tendency (probability or frequency) that a person helps a victim is high.\n2. The tendency of helping a victim decreases monotonically with the number of other people (bystanders) present.\n3. The tendency of helping a victim never drops to 0.\n\n**Task**: Sketch a first functional relationship that could model this phenomenon. Use the variables you defined in the previous step (including their labels and ranges).\n\n\n\n## Step 2: Define functional relationships between variables\nEvery causal path needs to be implemented as a mathematical function, where the dependent variable/output $y$ is a function of the input variable(s) $x_i$.\n\n$y = f(x_1, x_2, ..., x_i)$\n\nThis can, for example, be a linear function, $y = \\beta_0 + \\beta_1x_1$.\n\n## Step 2: Define functional relationships between variables\n### Fixed and free parameters\n\n::: {.smaller}\n$\\color{red} y = \\color{forestgreen} \\beta_0 \\color{black} + \\color{forestgreen} \\beta_1 \\color{blue} x$ \n<br>→ $\\color{red} y$ = output variable, $\\color{forestgreen} \\beta$s = parameters, $\\color{blue} x$ = input variable.\n\nTwo types of parameters:\n\n- **Fixed parameters**: Parameters that are chosen a priori and do not change in the light of empirical data. Their values are based on previous research, theory, or external information (such as properties of the world).\n- **Free parameters**: Can be adjusted to optimize the fit of the model to data. They are *estimated* from empirical data.\n:::\n\n::: {.callout-note}\nVirtually all parameters (except natural constants) could be imagined as being free. It is a *choice* to fix some of them in order to simplify the model.\n:::\n\n\n\n## Step 2: Define functional relationships between variables\n### Fixed and free parameters\n\nFixing a parameter:\n\n$\\color{forestgreen} \\beta_0 \\color{black} = 1 \\rightarrow \\color{red} y = \\color{forestgreen} 1 \\color{black} + \\color{forestgreen} \\beta_1 \\color{blue} x$ \n\nThat means, the slope $\\color{forestgreen} \\beta_1$ still can vary, but the intercept is fixed to 1.\n\n::: {.smaller}\nFree parameters give **flexibility** to your function: If you are unsure about the exact relationship between two variables, you can estimate the best-fitting parameters from the data.\n\nFor example, sometimes a theory specifies the general functional form of a relationship (e.g., \"With increasing $x$, $y$ is monotonously decreasing\"), but does not tell how fast this decrease happens, where $y$ starts when $x$ is minimal, etc. These latter decisions are then made by the free parameters. \n:::\n\n\n## {{< fa people-group size=2x >}} Discussion\n### Sketch a first functional relationship on paper\n\n- Which aspects of your sketched function could be free parameters? Describe them in plain language.\n- Draw a couple of alternative plots that (a) follow the same functional form and (b) also fullfill the criteria of the phenomenon. \n- What is the semantic meaning of the free parameters?\n\n\n<!-- TODO: Draw a handdrawn sketch of multiple curves. Three free parameters:\n1. Intercept: What is the base probability to help when nobody else is present? (Humans will differ in that probability)\n2. Slope: How fast does the probability decrease with each additional bystander? (Ths is the strength of the bystander effect)\n3. Lower asymptote: What is the minimum probability to help, even if there are many bystanders? (Humans will differ in that probability)\n -->\n\n\n\n<!-- ChatGPRT /Wolfram Alpha prompt\n\nCreate a function that maps values of x (which can go from 0 to infinity) to values of y (which goes from 0 to 1). It should start at a high value of y (this starting point should be free parameter of the function), decreases monotonically, and levels out at a certain value > 0 (which should also be a free parameter). A third free parameter is the speed of the decreasing.\nProvide plots for multiple instances of this function family.\n\n -->\n\n# Some mathematical tools\n\n## Tool 1: The logistic function family\n\nAs a linear function is unbounded, it can easily happen that the computed $y$ exceeds the possible range of values.\n\nIf $y$ has defined boundaries (e.g., $[0; 1]$), a logistic function can bound the values between a lower and an upper limit (in the basic logistic function, between 0 and 1):\n\n$y = \\frac{1}{1 + e^{-x}}$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n\n\n## Tool 1: The logistic function family\n\n::: {.smaller}\nWith the 4PL* model from IRT, you can adjust the functional form to your needs, by:\n\n- shifting the inflection point from left to right (aka. \"item difficulty\", parameter $d$)\n- change the steepness of the S-shape (aka. \"discrimination parameter\", parameter $a$)\n- Move the lower asymptote up or down (parameter $min$)\n- Move the upper limit up or down (parameter $max$)\n:::\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic <- function(x, d = 0, a = 1, min = 0, max = 1) {\n min + (max - min)/(1 + exp(a * (d - x)))\n}\n```\n:::\n\n\n\n\n\n\n::: footer\n*4PL = 4-parameter logistic model\n:::\n\n\n## Tool 1: The logistic function family\n\n*(basic logistic function as dotted grey line)*\n\n::: {.panel-tabset}\n#### Shift on x-axis\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n#### Steepness\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n#### Lower asymptote\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n#### Upper asymptote\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n\n\n\n## Tool 1: The logistic function family\n\nThe `d`, `a`, `min`, and `max` parameters can be used to \"squeeze\" the S-shaped curve into the range of your variables. For example, if your $x$ variable is defined on the range $[0; 1]$, the following function parameters lead to a reasonable shift:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n## Use the full mathematical toolbox\n\nOf course, the logistic function and the beta distribution are just a start - you can use the full toolbox of mathematical functions to implement your model!\n\n::: {.callout-note}\nThese considerations about functional forms, however, are typically not substantiated by psychological theory or background knowledge - at least at the start of a modeling project. We choose them, because we are (a) acquainted to it, and/or (b) they are mathematically convenient and tractable.\n\nEmpirical evidence can inform both your choice of the functional form, and, in a model-fitting step, the values of the parameters.\n:::\n\n\n\n# Step 3: Implement the functions in R\n\n## Excursus: Functions in R\n\n- **What is a function?**\n  - A reusable block of code designed to perform a specific task.\n  - Can be an implementation of an actual mathematical function $y = f(x, z)$\n  - But can also be a more complex operation, like reading a file, transforming data, or plotting a graph.\n- **Why use functions?**\n  - **Modularity:** Break down complex problems into manageable parts.\n  - **Reusability:** Write once, use multiple times.\n  - **Maintainability:** Easier to debug and update code.\n\n## Anatomy of an R Function\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Basic structure of a function\nmy_function <- function(arg1, arg2) {\n  # Function body\n  result <- arg1 + arg2\n  return(result)\n}\n```\n:::\n\n\n\n\n\n::: {.smaller}\n- **Function name:** `my_function`\n- **Parameters:** `arg1`, `arg2`\n- **Body:** Code that defines what the function does\n- **Return value:** return a specific value that has been computed in the function body with `return(return_variable)`. If no explicit `return()` statement is given, the last evaluated expression is returned by default.\n:::\n\n:::: {.columns .fragment}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# use meaningful names\n\nsum <- function(x, y) {\n  S <- x+y\n  return(S)\n}\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# short version: the last computation \n# is returned by default\n\nsum <- function(x, y) {x+y}\n```\n:::\n\n\n\n\n\n:::\n::::\n\n\n## Creating and Using Your Own Function\n\n1. **Define the Function:**\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_y <- function(x) {\n  y <- x^2\n  return(y)\n}\n```\n:::\n\n\n\n\n\n\n1. **Call the Function:**\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- get_y(x=2)\nprint(y)  # Output: 4\n```\n:::\n\n\n\n\n\n\n**Tips:**\n\n::: {.smaller}\n- Use meaningful and short names for functions and parameters.\n- Keep functions focused on a single task.\n- Document your functions with comments (ideally with the [`roxygen2`](https://roxygen2.r-lib.org) documentation standard)\n- Atomic functions: Each `R` function implements exactly *one* functional relationship of your model.\n:::\n\n\n## Example with roxygen2 documentation\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' Compute the updated expected anxiety\n#'\n#' The expected anxiety at any given moment is a weighted average of \n#' the momentary anxiety and the previous expected anxiety.\n#'\n#' @param momentary_anxiety The momentary anxiety, on a scale from 0 to 1\n#' @param previous_expected_anxiety The previous expected anxiety, on a scale from 0 to 1\n#' @param alpha A factor that shifts the weight between the momentary anxiety (alpha=1) \n#'              and the previous expected anxiety (alpha=0).\n#' @return The updated expected anxiety, as a scalar on a scale from 0 to 1\n\nget_expected_anxiety <- function(momentary_anxiety, previous_expected_anxiety, alpha=0.5) {\n  momentary_anxiety*alpha + previous_expected_anxiety*(1-alpha)\n}\n```\n:::\n\n\n\n\n\n\n::: {.smallest}\n`roxygen2` comments start with `#'` and are placed directly above the function definition.\n\n- The first line is the **title**\n- The following lines (after a blank line) are the **description**\n- Each **parameter** of the function is documented with `@param parameter_name Description`. Provide the range of possible values if applicable.\n- The **return value** is documented with `@return Description`\n:::\n\n## {{< fa people-group size=2x >}} Exercise: Document our exponential decay function\n\nCheck out [roxygen2](https://roxygen2.r-lib.org) and document our exponential decay function with: \n\n- title\n- description\n- parameters\n- return value.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_p_help <- function(NOPB, y_0, y_final, BSE_strength) {\n  p_help <- y_final + (y_0 - y_final) * exp(-BSE_strength * NOPB)\n  return(p_help)\n}\n```\n:::\n\n\n\n\n\n\n\n\n## {{< fa people-group size=1x >}} Apply everything to reduced model\n\n::: {.smallest}\n1. **Define** all variables of your model in the Variables Table (short name, range, scale level, semantic anchors). Variables\n2. **Categorize**: Which variables are exogenous, which are endogenous?\n   1. Exogenous variables are *not* influenced by other variables in the model.\n   2. Endogenous variables are influenced by other variables in the model.\n3. Every endogenous variable is a **function** of all of their input parameters. \n   1. Create sensible functional relationships for every endogenous variable.\n   2. Create a 3rd table (below the Construct Source Table and the Variables Table), with all functional relationships. \n   3. Add a short comment to what extent this functional relationship has been **derived from theory**: \n      1. Dictated by theory (add IDs from the Construct Source Table as reference)\n      2. Derived from theory\n      3. Loosely inspired by theory\n      4. Not based on focal theory, but rather on common sense or other theories\n   4. Add a short comment to what extent this functional relationship has been backed by empirical evidence.\n4. Implement the functions in `R` with proper `roxygen2` documentation.\n:::\n\n## Step 3: Implement the functions in R\n### Put everything together\n\nConnect all functions to one \"super-function\", which takes all exogenous variables as input and computes the focal output variable(s).\n\nTest the super-function:\n\n- Enter some reasonable combinations of parameters\n- Draw plots where you vary one parameter on the x-axis and see the behavior of the output variable on the y-axis.\n\n\n# Step 4 (Excursus): Fitting the functions to empirical data\n\n## Step 4 (Excursus): Fitting the functions to empirical data\n\n:::: {.columns}\n::: {.column width='60%' .r-fit-text}\nWe can tune our free parameters to fit the model as good as possible to empirical data. This is called **model fitting**.\n\nSee the [Find-a-fit app](https://shinyapps.org/showapp.php?app=https://shiny.psy.lmu.de/felix/lmfit&by=Felix%20Schönbrodt&title=Find-a-fit!&shorttitle=Find-a-fit!) for an example of a simple linear regression with two parameters (intercept and slope) that are fitted by an optimization algorithm: \n\n:::\n\n::: {.column width='40%'}\n![](img/Find-a-fit.png){height=40% .ds}\n:::\n:::: \n\n\n# Step 5: Simulate a full data set\n\n## Step 5: Simulate a full data set\n### 5a: Create a design matrix for experimental factors\n\nCreate a design matrix for all possible combinations of experimental factors (i.e., those variables that you control/manipulate at specific levels).\n\nThe `expand.grid()` function in `R` comes as a handy tool for fully crossed factors (the first factor varies fastest, the last factor varies slowest):\n\n:::: {.columns}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add all factors as arguments:\ndf <- expand.grid(\n  F1 = c(\"A\", \"B\"), \n  F2 = c(1, 2, 3),\n  F3 = c(TRUE, FALSE)\n)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   F1 F2    F3\n1   A  1  TRUE\n2   B  1  TRUE\n3   A  2  TRUE\n4   B  2  TRUE\n5   A  3  TRUE\n6   B  3  TRUE\n7   A  1 FALSE\n8   B  1 FALSE\n9   A  2 FALSE\n10  B  2 FALSE\n11  A  3 FALSE\n12  B  3 FALSE\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n::::\n\n\n## Step 5: Simulate a full data set\n### 5a: Create a design matrix for experimental factors\n\n::: {.smaller}\nTo create a virtual sample, add one additional variable with a participant ID (this also determines the size of your sample):\n\n:::: {.columns}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_per_condition <- 3\n\ndf <- expand.grid(\n  pID     = 1:n_per_condition, \n  valence = c(\"pos\", \"neg\"), \n  speed   = c(\"slow\", \"fast\")\n)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   pID valence speed\n1    1     pos  slow\n2    2     pos  slow\n3    3     pos  slow\n4    1     neg  slow\n5    2     neg  slow\n6    3     neg  slow\n7    1     pos  fast\n8    2     pos  fast\n9    3     pos  fast\n10   1     neg  fast\n11   2     neg  fast\n12   3     neg  fast\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n::::\n\nWe have 12 participants overall: 3 participants in the `pos/slow` condition, 3 in the `neg/slow` condition, and so forth.\nNote that, although the participant ID repeats within each condition, these could be different (independent) participants if we assume a between-person design.\n:::\n\n\n## Step 5: Simulate a full data set\n### 5b: Simulate random observed variables\n\nAdd observed interindividual differences or situational variables. These are not experimentally fixed at specific levels, but vary randomly between participants:\n\n:::: {.columns}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- nrow(df)\ndf$age <- rnorm(n, mean=26, sd=4) |> round()\n\n# Extraversion\n# z-scores: standard normal distribution\ndf$extra <- rnorm(n) |> round(1) \n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   pID valence speed age extra\n1    1     pos  slow  17   1.1\n2    2     pos  slow  17   2.8\n3    3     pos  slow  23   1.5\n4    1     neg  slow  28   0.6\n5    2     neg  slow  25   0.7\n6    3     neg  slow  29   0.5\n7    1     pos  fast  23   0.1\n8    2     pos  fast  27  -2.0\n9    3     pos  fast  26   1.0\n10   1     neg  fast  32  -0.5\n11   2     neg  fast  22   0.2\n12   3     neg  fast  25  -1.4\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n::::\n\n\n## Step 5: Simulate a full data set\n### 5c: Compute the outcome variable (i.e., the model output)\n\nOnce all input variables have been simulated, submit them to your model function and compute the outcome variable $y$:\n\n:::: {.columns}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# psi() is the model function that takes \n# all input parameters and returns the \n# simulated output\n\ndf$y <- psi(df$valence, df$speed, \n            df$age, df$extra)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   pID valence speed age extra    y\n1    1     pos  slow  17   1.1  6.2\n2    2     pos  slow  17   2.8 10.5\n3    3     pos  slow  23   1.5 12.0\n4    1     neg  slow  28   0.6  9.3\n5    2     neg  slow  25   0.7  8.1\n6    3     neg  slow  29   0.5  9.7\n7    1     pos  fast  23   0.1  8.2\n8    2     pos  fast  27  -2.0  7.5\n9    3     pos  fast  26   1.0  8.2\n10   1     neg  fast  32  -0.5  8.1\n11   2     neg  fast  22   0.2  8.4\n12   3     neg  fast  25  -1.4 12.1\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n::::\n\nMake sure that the `psi()` function can handle vectors as input (i.e., you can submit the entire data frame of input variables to the function).\n\n\n## Step 5: Simulate a full data set\n### Sources of interindividual differences\n\nNot every person is identical, so in reality there probably are interindividual differences at several places:\n\n- in the sensors (i.e., manipulations and perceptions do not work the same for everyone)\n- in the actors (i.e., people differ how internal impulses are translated into overt behavior)\n- interindividual difference variables in the model\n- Different parameter values in the functional relationships. For example, the individual treatment effect (ITE) could vary between participants.\n\nWe **can** model these interindividual differences - or we assume that some of them are constant for all participants.\n\n## Step 5: Simulate a full data set\n### 5c: Unexplained variance / random noise\n\n::: {.smaller}\nFurthermore, our models are always simplifications of reality. We can never model all relevant variables; and measurement error adds further noise. Consequently there is some **unexplained variability** (aka. random noise) in the system. \n\nAll additional sources of variation could be modeled as a single random error term pointing to the final outcome variable. This summarizes all additional sources of variation that are not explicitly modeled:\n:::\n\n::: {.svg-h200 style=\"text-align: center;\"}\n\n\n\n\n\n```{dot}\ndigraph G {\n    // Use neato layout engine to allow manual positioning\n    layout=neato;\n\n    // Nodes with explicit (x, y) positions\n    X [pos=\"0,0!\", shape=box];\n    Y [pos=\"2,0!\", shape=box];\n    // Invisible starting point of empty arrow\n    start [shape=point, style=invis, pos=\"2,1!\"];\n\n    // Define edges with one-sided arrows\n    X -> Y [xlabel=\"c  \"];\n    start -> Y [color=red];\n}\n```\n\n\n\n\n\n:::\n\n\n\n## Step 5: Simulate a full data set\n### 5c: Compute the outcome variable (i.e., the model output)\n\nAdd additional (summative) error variance to output variable:\n\n:::: {.columns}\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# deterministic model output\ndf$y <- psi(df$valence, df$speed, \n            df$age, df$extra)\n\n# Add additional noise to observed variable.\n# The SD of the normal distribution\n# determines the amount of error\ndf$y_obs <- df$y + rnorm(n, mean=0, sd=2) |> \n            round(1)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width='50%'}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   pID valence speed age extra    y y_obs\n1    1     pos  slow  17   1.1  6.2   7.8\n2    2     pos  slow  17   2.8 10.5  14.8\n3    3     pos  slow  23   1.5 12.0  10.4\n4    1     neg  slow  28   0.6  9.3  10.4\n5    2     neg  slow  25   0.7  8.1   8.5\n6    3     neg  slow  29   0.5  9.7  10.7\n7    1     pos  fast  23   0.1  8.2  11.0\n8    2     pos  fast  27  -2.0  7.5  10.1\n9    3     pos  fast  26   1.0  8.2   6.8\n10   1     neg  fast  32  -0.5  8.1   8.8\n11   2     neg  fast  22   0.2  8.4  11.0\n12   3     neg  fast  25  -1.4 12.1  12.2\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n::::\n\nThe size of the error variance (in combination with upstream sources of interindividual variance) determines the effect size that can be observed in a simulated study. The more error variance, the lower the effect size.\n\n\n\n\n## {{< fa people-group size=2x >}} Group work (20 min.):<br>Create a design matrix\n\n::: {.smaller}\n1. In your own `R` project, create a design matrix with `expand.grid()` and the following fully crossed experimental factors ($n=30$ participants per condition):\n   - Danger of intervention (`DoI`) with the levels 0.2 and 0.8\n   - Number of passive bystanders (`NOPB`) with the levels 0, 1, and 4\n2. Add random variables for all exogenous\\* variables of your model that are not experimentally manipulated. \n   - This includes, e.g., the personal base resposibility, `baseResp`. \n   - Furthermore, add neuroticism (`neuro`) and age (`age`). (Note: We don't need them for our model, just for practice.)\n   - Think about justifiable settings for the simulated variables (e.g., type of distribution, range, mean, SD).\n3. (Do *not* compute the output variable yet)\n4. Push everything to the repository.\n:::\n\n::: footer\n\\* Reminder: Exogenous variables are those variables in your model that have no arrows pointing towards them.\n:::\n\n\n\n\n## Tool 3: The beta distribution\n\nA handy distribution for the $[0; 1]$ range is the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution). With its two parameters $\\alpha$ (also called $a$ or `shape1`) and $\\beta$ (also called $b$ or `shape2`), it can take many different forms:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n\n## Tool 3: The beta distribution\n\n**How to choose $\\alpha$ and $\\beta$? Asking ChatGPT/Wolfram Alpha for assistance**\n\nAssume that you intuitively started with a normal distribution with $M=0.2$ and $SD=0.1$ (`rnorm(mean=0.2, sd=0.1)`) to simulate your values. But this function can generate values < 0 and > 1. \n\nHow can you achieve a beta distribution that approximates the properties of this normal distribution? You can work out the math yourself (e.g., by looking at the formula for the mean and the variance of the beta distribution) - or you can ask ChatGPT. After all, we only use this function as a tool to get some plausible values.\n\n\n## Tool 3: The beta distribution\n\n::: {.panel-tabset}\n### Prompt (Choose Wolfram Alpha as ChatGPT plugin):\n> \"We have a normal distribution with mean=0.2 and SD=0.1. But the results should be bounded on a scale from 0 to 1. Create a beta-distribution that mimics the properties of the described normal distribution.\"\n\n### Wolfram Alpha answer:\n::: {.smaller}\n> \"To mimic the properties of a specified normal distribution (with a mean and standard deviation) using a beta distribution within a bounded interval (in this case, 0 to 1), we need to find the parameters of the beta distribution (alpha $\\alpha$ and beta $\\beta$) that match the mean and variance of the normal distribution as closely as possible. \n> \n>  [snip]\n> \n> The parameters for the beta distribution that mimic the properties of the described normal distribution (with mean = 0.2 and standard deviation = 0.1, bounded between 0 and 1) are $\\alpha = 3$ and $\\beta = 12$.\n> \n> This beta distribution should closely match the shape and spread of the specified normal distribution within the bounds of 0 to 1.\"\n:::\n:::\n\n\n\n## Tool 3: The beta distribution\n### Approximating a normal distribution with a beta distibution\n\nYou can generate random values in `R` with the `rbeta` function. Here's a comparison of a normal distribution and a matched beta distribution that respects the boundaries $[0; 1]$:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx.random <- rnorm(10000, mean=0.2, sd=0.1)\nhist(x.random, xlab = \"\", ylab=\"\", main=\"Normal distribution (M=0.2, SD=0.1)\", xlim=c(-0.3, 1.1))\n\nx.beta <- rbeta(10000, shape1=3, shape2=12)\nhist(x.beta, xlab = \"\", ylab=\"\", main=\"beta distribution (a=3, b=12)\", xlim=c(-0.3, 1.1))\n```\n:::\n\n\n\n\n\n\n\n## Tool 3: The beta distribution\n### Approximating a normal distribution with a beta distibution\n\n\n\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](MathModel_files/figure-html/unnamed-chunk-28-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n\n\n## Tool: The Distribution Zoo\n\nIf you start simulating data for your virtual participants, you draw random values from a distribution. For example, the virtual participants might differ in their anxiety, which you previously defined on the range $[0; 1]$. \n\nHow can you generate random values that roughly look like a normal distribution, but are bounded to the defined range?\n\nFor simulations, it is good to know some basic distributions. Here are three interactive resources for choosing your distribution:\n\n- The [Distribution Zoo](https://ben18785.shinyapps.io/distribution-zoo/) by Ben Lambert and Fergus Cooper\n- The [Probability Distribution Explorer](https://distribution-explorer.github.io/index.html) by Justin Bois\n- [Interactive collection of distributions](https://richarddmorey.github.io/stat-distributions-js/) by Richard Morey\n\n\n\n## {{< fa people-group size=2x >}} Group work (45 min.):<br>Full simulation\n\nBased on your design matrix from the previous exercise:\n\n3. Compute the output variable of your model for each participant. Optionally: Add additional noise to the output variable.\n4. Visualize the simulated data (e.g., with `ggplot2`).\n5. Push everything to the repository.\n\n::: footer\n\\* Reminder: Exogenous variables are those variables in your model that have no arrows pointing towards them.\n:::\n\n\n## Step 5: Simulate a full data set\n\nSimulate this design, analog to Fischer et al. (2006): \"A 2 (bystander: yes vs. no) x 2 (danger: low vs high) factorial design was employed.\"\n\n![](img/study_to_simulate.png){height=250}\n\n\n- Does the simulated model produce the phenomenon? How large is the effect size?\n- What happens if you change some of the functional parameters? Is the phenomenon still there? Is there a point in parameter space where the phenomenon breaks down?\n\n\n# Evaluation of the model\n\n## Evaluation of the model\n### When does the formal model produce the phenomenon?\n\n\"one could conduct this simulation with very large sample sizes and use a statistical function for an effect size, like Cohen’s d, to express the results. Any effect size that would be detectable with “realistic sample sizes” could then count as production of the statistical pattern and as such be used to evaluate robustness.\"\n\n<!-- Footer insert below -->\n\n\n\n\n\n\n# End\n\n## Contact\n\n<script src=\"https://kit.fontawesome.com/9fb269b0d2.js\" crossorigin=\"anonymous\"></script>\n\n<ul class=\"fa-ul\" style=\"color:black; list-style:none;\">\n\n<li><i class=\"fa-brands fa-li fa-mastodon\"></i> <a href=\"https://scicomm.xyz/@nicebread\" target=\"_blank\" style=\"color:black; border-bottom:none;\">@nicebread@scicomm.xyz</a></li>\n\n<li><i class=\"fa-li fa fa-envelope-o\"></i> <a style=\"unicode-bidi:bidi-override; direction: rtl; color:black\" href=\"javascript:window.location.href = 'mailto:' + ['felix.schoenbrodt','psy.lmu.de'].join('@')\">ed.uml.ysp@tdorbneohcs.xilef</a></li>\n\n<li><i class=\"fa-li fa-solid fa-globe\"></i> <a href=\"https://www.nicebread.de\" target=\"_blank\" style=\"color:black; border-bottom:none;\">https://www.nicebread.de</a></li>\n\n\n<li><i class=\"fa-li fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/nicebread\" target=\"_blank\" style=\"color:black; border-bottom:none;\">https://github.com/nicebread</a></li>\n</ul>\n\n<small style=\"text-align:left;\">\n\n\n[![CC-BY-SA 4.0][cc-by-sa-image]][cc-by-sa]\n\n[cc-by-sa]: http://creativecommons.org/licenses/by-sa/4.0/\n[cc-by-sa-image]: https://licensebuttons.net/l/by-sa/4.0/88x31.png\n[cc-by-sa-shield]: https://img.shields.io/badge/License-CC%20BY%20SA4.0-lightgrey.svg\n\n</small>\n\n\n\n\n\n\n\n<!-- SPEICHER\n\n\n## Step 5: Simulate a full data set\n\n- If your model is deterministic, the same input parameters will always produce exactly the same output.\n- In real data sets, however, we see a huge variability in the output variables. **Where does the variability come from?**\n  \n## Step 5: Simulate a full data set\n\n::: {.smaller}\n\nIf we assume that our model is the **true data generating model**, i.e.:\n\n- we modeled all relevant input variables (none is missing) \n- we got all functional relationships right\n\n**and** we assume that:\n\n- all model parameters are fixed (i.e., identical for all participants - there are no interindividual differences)\n\n... then we still see variability in the output variables. This can come from:\n\n- Participants bring different values in their exogenous variables\n- Measurement error in all measured variables, or probabilistic processes in the system\n:::\n\n -->",
    "supporting": [
      "MathModel_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}